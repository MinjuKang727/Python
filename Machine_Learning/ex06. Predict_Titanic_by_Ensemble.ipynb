{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1311aa75",
   "metadata": {},
   "source": [
    "# 앙상블\n",
    "- 교재 : 210 ~ 250쪽 참고\n",
    "\n",
    "- 보팅(Voting)\n",
    "    - 서로 다른 알고리즘의 모델 여러개를 이용해서 투표로 예측 결과를 만들어 낸다.\n",
    "    - 하드보팅 : 모델의 최종 예측 결과로 투표\n",
    "    - 소프트보팅 : 모델의 확률결과로 투표\n",
    "- 배깅(Bagging)\n",
    "    - 같은 알고리즘 모델 여러개를 이용해서 투표를 진행\n",
    "    - 여러개의 모델들이 학습할 때, 다른 의견이 나올 수 있도록 부트 스트랩핑이라는 추출 방법을 사용\n",
    "    - 매번 모델이 학습할 때마다 랜덤하게 데이터를 추출하되, 중복된 추출을 허용한다.\n",
    "    - 랜덤포레스트 : DecisionTreee의 배깅 모델\n",
    "- 부스팅(Boosting)\n",
    "    - 같은 알고리즘 모델 여러개를 이용해서 투표를 진행\n",
    "    - 이전 모델이 잘못 학습한 결과를 다음 모델이 반영해서 학습하도록 하는 기법\n",
    "    - 학습 진행이 Sequencial해서(순차적으로 일어남) 상대적으로 학습 속도가 느리다.\n",
    "    - 교재 222 쪽 참고\n",
    "- 스택킹(Stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d3eeb",
   "metadata": {},
   "source": [
    "# Random Forest(랜덤 포레스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e73c0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "X_train = pd.read_csv('data/X_train.csv')\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "submission = pd.read_csv('data/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb927fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39c52df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "        n_estimators = 100,   # 깅할 모델 갯수\n",
    "        max_features = 0.5,   # 각 모델이 랜덤하게 사용할 특성 수\n",
    "        max_depth = 8,        # 각 모델의 최대 깊이\n",
    "        min_samples_leaf = 10,# 리프 노드의 최소 샘플 수\n",
    "        random_state = 72     # 난수 고정\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b1c0b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.83798883, 0.80898876, 0.85393258, 0.79213483, 0.84269663])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf_rs = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
    "rf_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c17609",
   "metadata": {},
   "source": [
    "# 그리드 서치\n",
    "- 앙상블 모델의 경우 하이퍼파리미터 수 가 엉청 많다\n",
    "- 각 파라미터를 독립접으로 튜닝하느 건 불가능\n",
    "- 많은 양의 파라미터를 동시에 튜닝하기 위한 함수로 그리드 서치를 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ee1d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e45d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐색하고 싶은 하이퍼파라미터 조합을 딕셔너리로 생성\n",
    "param_grid = {\n",
    "    'max_depth' : [5, 10, 15, 20],\n",
    "    'n_estimators' : [100, 150, 200, 250],\n",
    "    'max_features' : [0.5, 0.7],\n",
    "    'min_samples_leaf' : [10, 20, 25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37c5d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 720)\n",
    "# 튜닝할 모델, 사용할 파라미터, 교차검증 횟수\n",
    "grid = GridSearchCV(model,param_grid, cv=3, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66295e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=720),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 10, 15, 20],\n",
       "                         'max_features': [0.5, 0.7],\n",
       "                         'min_samples_leaf': [10, 20, 25],\n",
       "                         'n_estimators': [100, 150, 200, 250]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5a02168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8327721661054994"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최고의 조합 확인\n",
    "# best_score : 최고 조합의 정확도 반환\n",
    "# best_params : 최고 조합의 매개 변수 반환\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75d3fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 조합으로 학습 완료된 모델\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f75f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3a4fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Survived'] = pre\n",
    "submission.to_csv('data/rf_pre.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412c155",
   "metadata": {},
   "source": [
    "### XG부스트\n",
    "- 교재 255쪽 : XG 부스트 - 성능만 극대화 시키기 위한 모델 (부스팅 모델의 느린 속도를 많이 보완)\n",
    "- 교재 258쪽 : XG 부스트 하이퍼 파라미터\n",
    "\n",
    "### 라이트GBM\n",
    "- 교재 244쪽 : 라이트GBM - XG부스트보다 속도를 조금 더 개선\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e64dc0",
   "metadata": {},
   "source": [
    "# Boosting 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b099623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-win_amd64.whl (125.4 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\ai\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ai\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.1\n"
     ]
    }
   ],
   "source": [
    "# XG부스트 설치\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40fc4118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\ai\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\ai\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\ai\\anaconda3\\lib\\site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ai\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\ai\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ai\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ai\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# 라이트GBM 설치\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d718cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a0d5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 종류(주로 쓰는 것) \n",
    "# booster = gbtree(기본값) 또는 gblinesr 또는 dart\n",
    "# maxdepth\n",
    "# subsmaple\n",
    "# n_estimators\n",
    "# learning_rate : 이전에 잘못 판단한 것을 얼마나 반영할 것인지? (값이 높을수록 많이 반영)\n",
    "# gammma나 alpha 는 linear모델일 때는 사용X\n",
    "xgb_model = XGBClassifier(\n",
    "    booster='gbtree',    # 모델의 종류\n",
    "    n_estimators = 100,  # 앙상블 모델 갯수\n",
    "    learning_rate = 0.1, # 학습률\n",
    "    max_depth = 4,       # 최대 깊이(일반적으로 깊게 설정X)\n",
    "    gamma = 0.2          # 키우면 모델 단순, 작아지면 모델이 복잡\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52f75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
